{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Scanning repository for aggregated JSON ...\n",
      "[i] Files matched: 2775\n",
      "   • data/aggregated/transaction/country/india/2022/1.json\n",
      "   • data/aggregated/transaction/country/india/2022/2.json\n",
      "   • data/aggregated/transaction/country/india/2022/3.json\n",
      "   • data/aggregated/transaction/country/india/2022/4.json\n",
      "   • data/aggregated/transaction/country/india/2024/1.json\n",
      "   • data/aggregated/transaction/country/india/2024/2.json\n",
      "   • data/aggregated/transaction/country/india/2024/3.json\n",
      "   • data/aggregated/transaction/country/india/2024/4.json\n",
      "   • data/aggregated/transaction/country/india/2023/1.json\n",
      "   • data/aggregated/transaction/country/india/2023/2.json\n",
      "[OK] Wrote outputs/aggregated_insurance_country.csv  (19 rows)\n",
      "[OK] Wrote outputs/aggregated_insurance_state.csv  (682 rows)\n",
      "[OK] Wrote outputs/aggregated_transaction_country.csv  (140 rows)\n",
      "[OK] Wrote outputs/aggregated_transaction_state.csv  (5,034 rows)\n",
      "[OK] Wrote outputs/aggregated_user_country.csv  (215 rows)\n",
      "[OK] Wrote outputs/aggregated_user_state.csv  (7,740 rows)\n",
      "[i] Found 36 state folders:\n",
      "   - andaman-&-nicobar-islands\n",
      "   - andhra-pradesh\n",
      "   - arunachal-pradesh\n",
      "   - assam\n",
      "   - bihar\n",
      "   - chandigarh\n",
      "   - chhattisgarh\n",
      "   - dadra-&-nagar-haveli-&-daman-&-diu\n",
      "   - delhi\n",
      "   - goa\n",
      "   - gujarat\n",
      "   - haryana\n",
      "   - himachal-pradesh\n",
      "   - jammu-&-kashmir\n",
      "   - jharkhand\n",
      "   - karnataka\n",
      "   - kerala\n",
      "   - ladakh\n",
      "   - lakshadweep\n",
      "   - madhya-pradesh\n",
      "   - maharashtra\n",
      "   - manipur\n",
      "   - meghalaya\n",
      "   - mizoram\n",
      "   - nagaland\n",
      "   - odisha\n",
      "   - puducherry\n",
      "   - punjab\n",
      "   - rajasthan\n",
      "   - sikkim\n",
      "   - tamil-nadu\n",
      "   - telangana\n",
      "   - tripura\n",
      "   - uttar-pradesh\n",
      "   - uttarakhand\n",
      "   - west-bengal\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DATA_ROOT = Path(\"data\")\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SECTION = \"aggregated\"  # we only handle 'aggregated' in this script\n",
    "SAVE_FORMAT = \"csv\"     # 'csv' or 'parquet'\n",
    "\n",
    "# Accept plural/singular folder names and normalize\n",
    "TYPE_ALIASES = {\n",
    "    \"transaction\": \"transaction\",\n",
    "    \"transactions\": \"transaction\",\n",
    "    \"user\": \"user\",\n",
    "    \"users\": \"user\",\n",
    "    \"insurance\": \"insurance\",\n",
    "    \"insurances\": \"insurance\",\n",
    "}\n",
    "\n",
    "# -------------- Helpers ---------------\n",
    "def _safe_get(d: dict, *keys, default=None):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def detect_type_folders() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Returns mapping of actual folder name -> normalized type ('transaction'|'user'|'insurance')\n",
    "    under data/aggregated/* .\n",
    "    \"\"\"\n",
    "    m: Dict[str, str] = {}\n",
    "    root = DATA_ROOT / SECTION\n",
    "    if not root.exists():\n",
    "        return m\n",
    "    for child in root.iterdir():\n",
    "        if child.is_dir():\n",
    "            norm = TYPE_ALIASES.get(child.name.lower())\n",
    "            if norm:\n",
    "                m[child.name] = norm\n",
    "    return m\n",
    "\n",
    "def parse_path(p: Path, norm_type_by_folder: Dict[str, str]) -> Optional[Dict[str, Optional[str]]]:\n",
    "    \"\"\"\n",
    "    Supports (corrected state path):\n",
    "      data/aggregated/<typeFolder>/country/india/<year>/<q>.json\n",
    "      data/aggregated/<typeFolder>/country/india/state/<state>/<year>/<q>.json\n",
    "    where <typeFolder> can be singular or plural; normalized to {'transaction','user','insurance'}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = p.parts\n",
    "        if \"data\" not in parts or \"aggregated\" not in parts:\n",
    "            return None\n",
    "        i = parts.index(\"data\")\n",
    "        if parts[i+1].lower() != \"aggregated\":\n",
    "            return None\n",
    "\n",
    "        type_folder = parts[i+2]\n",
    "        typ = norm_type_by_folder.get(type_folder)\n",
    "        if not typ:\n",
    "            return None\n",
    "\n",
    "        if parts[i+3].lower() != \"country\" or parts[i+4].lower() != \"india\":\n",
    "            return None\n",
    "\n",
    "        if not p.name.endswith(\".json\"):\n",
    "            return None\n",
    "        quarter = p.stem\n",
    "        if quarter not in {\"1\",\"2\",\"3\",\"4\"}:\n",
    "            return None\n",
    "\n",
    "        remainder = parts[i+5:-1]  # after 'india' up to file\n",
    "        if len(remainder) == 1:\n",
    "            year = remainder[0]\n",
    "            geo_level = \"country\"\n",
    "            state = None\n",
    "        elif len(remainder) == 3 and remainder[0].lower() == \"state\":\n",
    "            state = remainder[1]\n",
    "            year = remainder[2]\n",
    "            geo_level = \"state\"\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        if not year.isdigit():\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"section\": \"aggregated\",\n",
    "            \"type\": typ,\n",
    "            \"geo_level\": geo_level,\n",
    "            \"country\": \"india\",\n",
    "            \"state\": state,\n",
    "            \"year\": int(year),\n",
    "            \"quarter\": int(quarter),\n",
    "            \"source_path\": str(p),\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def iter_json_files() -> List[Tuple[Path, Dict[str, Optional[str]]]]:\n",
    "    files = []\n",
    "    type_map = detect_type_folders()\n",
    "    # if nothing detected, still try known folders\n",
    "    if not type_map:\n",
    "        for fallback in [\"transaction\", \"transactions\", \"user\", \"users\", \"insurance\"]:\n",
    "            type_map[fallback] = TYPE_ALIASES.get(fallback)\n",
    "    for type_folder, norm_type in type_map.items():\n",
    "        # Country\n",
    "        for p in (DATA_ROOT / SECTION / type_folder / \"country\" / \"india\").glob(\"*/*.json\"):\n",
    "            meta = parse_path(p, type_map)\n",
    "            if meta:\n",
    "                files.append((p, meta))\n",
    "        # State\n",
    "        for p in (DATA_ROOT / SECTION / type_folder / \"country\" / \"india\" / \"state\").glob(\"*/*/*.json\"):\n",
    "            meta = parse_path(p, type_map)\n",
    "            if meta:\n",
    "                files.append((p, meta))\n",
    "    return files\n",
    "\n",
    "# -------------- Normalizers ---------------\n",
    "def normalize_transaction_or_insurance(payload: dict, base: Dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    items = _safe_get(payload, \"data\", \"transactionData\", default=[]) or []\n",
    "    for item in items:\n",
    "        name = item.get(\"name\")\n",
    "        instruments = item.get(\"paymentInstruments\") or []\n",
    "        total = next((pi for pi in instruments if (pi.get(\"type\") or \"\").upper() == \"TOTAL\"), None)\n",
    "        count = None if total is None else total.get(\"count\")\n",
    "        amount = None if total is None else total.get(\"amount\")\n",
    "        rows.append({\n",
    "            **base,\n",
    "            \"category\": name,\n",
    "            \"count\": count,\n",
    "            \"amount\": amount,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def normalize_user(payload: dict, base: Dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    agg = _safe_get(payload, \"data\", \"aggregated\", default={}) or {}\n",
    "    rows.append({\n",
    "        **base,\n",
    "        \"brand\": None,\n",
    "        \"registeredUsers\": agg.get(\"registeredUsers\"),\n",
    "        \"appOpens\": agg.get(\"appOpens\"),\n",
    "        \"brand_count\": None,\n",
    "        \"brand_percentage\": None,\n",
    "    })\n",
    "    by_device = _safe_get(payload, \"data\", \"usersByDevice\", default=[]) or []\n",
    "    for d in by_device:\n",
    "        rows.append({\n",
    "            **base,\n",
    "            \"brand\": d.get(\"brand\"),\n",
    "            \"registeredUsers\": None,\n",
    "            \"appOpens\": None,\n",
    "            \"brand_count\": d.get(\"count\"),\n",
    "            \"brand_percentage\": d.get(\"percentage\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -------------- Build ----------------\n",
    "def collect_frames() -> Dict[Tuple[str, str], pd.DataFrame]:\n",
    "    buckets: Dict[Tuple[str, str], List[pd.DataFrame]] = {}\n",
    "    matched = iter_json_files()\n",
    "\n",
    "    # quick report\n",
    "    print(\"[i] Files matched:\", len(matched))\n",
    "    sample = [m[0] for m in matched[:10]]\n",
    "    for s in sample:\n",
    "        print(\"   •\", s)\n",
    "\n",
    "    for path, meta in matched:\n",
    "        base = {\n",
    "            \"section\": meta[\"section\"],\n",
    "            \"type\": meta[\"type\"],\n",
    "            \"geo_level\": meta[\"geo_level\"],\n",
    "            \"country\": meta[\"country\"],\n",
    "            \"state\": meta[\"state\"],\n",
    "            \"year\": meta[\"year\"],\n",
    "            \"quarter\": meta[\"quarter\"],\n",
    "            \"period\": f'{meta[\"year\"]}-Q{meta[\"quarter\"]}',\n",
    "            \"source_path\": meta[\"source_path\"],\n",
    "        }\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                payload = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if meta[\"type\"] in (\"transaction\", \"insurance\"):\n",
    "            df = normalize_transaction_or_insurance(payload, base)\n",
    "        else:\n",
    "            df = normalize_user(payload, base)\n",
    "\n",
    "        buckets.setdefault((meta[\"type\"], meta[\"geo_level\"]), []).append(df)\n",
    "\n",
    "    out: Dict[Tuple[str, str], pd.DataFrame] = {}\n",
    "    for key, frames in buckets.items():\n",
    "        if not frames:\n",
    "            continue\n",
    "        df = pd.concat(frames, ignore_index=True)\n",
    "        common = [\"section\",\"type\",\"geo_level\",\"country\",\"state\",\"year\",\"quarter\",\"period\",\"source_path\"]\n",
    "        optionals = [\"category\",\"count\",\"amount\",\"brand\",\"registeredUsers\",\"appOpens\",\"brand_count\",\"brand_percentage\"]\n",
    "        for c in optionals:\n",
    "            if c not in df.columns:\n",
    "                df[c] = pd.Series([None]*len(df))\n",
    "        df = df[common + optionals]\n",
    "        out[key] = df.sort_values(\n",
    "            [\"type\",\"geo_level\",\"state\",\"year\",\"quarter\",\"category\",\"brand\"],\n",
    "            na_position=\"last\"\n",
    "        ).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def save_outputs(dfs: Dict[Tuple[str, str], pd.DataFrame]) -> None:\n",
    "    wanted = [\n",
    "        (\"insurance\",\"country\"),\n",
    "        (\"insurance\",\"state\"),\n",
    "        (\"transaction\",\"country\"),\n",
    "        (\"transaction\",\"state\"),\n",
    "        (\"user\",\"country\"),\n",
    "        (\"user\",\"state\"),\n",
    "    ]\n",
    "    for typ, geo in wanted:\n",
    "        df = dfs.get((typ, geo), pd.DataFrame())\n",
    "        fname = OUTPUT_DIR / f\"aggregated_{typ}_{geo}.{ 'parquet' if SAVE_FORMAT=='parquet' else 'csv' }\"\n",
    "        if df.empty:\n",
    "            print(f\"[WARN] No rows for ({typ}, {geo}). Writing an empty file header to {fname}.\")\n",
    "            if SAVE_FORMAT == \"parquet\":\n",
    "                # Parquet doesn't like purely empty w/o schema; write CSV header instead\n",
    "                df.to_csv(fname.with_suffix(\".csv\"), index=False)\n",
    "            else:\n",
    "                df.to_csv(fname, index=False)\n",
    "            continue\n",
    "        if SAVE_FORMAT == \"parquet\":\n",
    "            df.to_parquet(fname, index=False)\n",
    "        else:\n",
    "            df.to_csv(fname, index=False)\n",
    "        print(f\"[OK] Wrote {fname}  ({len(df):,} rows)\")\n",
    "\n",
    "def list_state_folders() -> List[str]:\n",
    "    names = set()\n",
    "    root = DATA_ROOT / SECTION\n",
    "    if not root.exists():\n",
    "        return []\n",
    "    for type_folder in root.iterdir():\n",
    "        if not type_folder.is_dir():\n",
    "            continue\n",
    "        if TYPE_ALIASES.get(type_folder.name.lower()) is None:\n",
    "            continue\n",
    "        state_root = type_folder / \"country\" / \"india\" / \"state\"\n",
    "        if state_root.exists():\n",
    "            for s in state_root.iterdir():\n",
    "                if s.is_dir():\n",
    "                    names.add(s.name)\n",
    "    return sorted(names)\n",
    "\n",
    "# -------------- Main ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"[i] Scanning repository for aggregated JSON ...\")\n",
    "    dfs = collect_frames()\n",
    "    save_outputs(dfs)\n",
    "    states = list_state_folders()\n",
    "    if states:\n",
    "        print(f\"[i] Found {len(states)} state folders:\")\n",
    "        for s in states:\n",
    "            print(\"   -\", s)\n",
    "    else:\n",
    "        print(\"[i] No state folders found under aggregated/*/country/india/state/*\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Mar  1 2023, 12:33:47) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37adc53838b3cbe9373496bf8f5d0279b2162a98891d7e03792d3d7d15ca8698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
